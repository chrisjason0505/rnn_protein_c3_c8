{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install_deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install wandb transformers torch datasets -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f6727e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# ====== TRACKIO/WANDB INTEGRATION ======\n",
                "import wandb\n",
                "# Initialize Weights & Biases for experiment tracking\n",
                "wandb.init(\n",
                "    project=\"protein-secondary-structure\",\n",
                "    name=\"bilstm-sst-prediction\",\n",
                "    config={\n",
                "        \"batch_size\": 32,\n",
                "        \"embed_dim\": 64,\n",
                "        \"hidden_dim\": 128,\n",
                "        \"epochs\": 10,\n",
                "        \"learning_rate\": 1e-3\n",
                "    }\n",
                ")\n",
                "\n",
                "# ====== HUGGING FACE INTEGRATION ======\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "# We'll use ProtBERT embeddings as an option\n",
                "# Uncomment below to use pre-trained protein embeddings\n",
                "# hf_model_name = \"Rostlab/prot_bert\"\n",
                "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
                "# hf_model = AutoModel.from_pretrained(hf_model_name)\n",
                "\n",
                "print(\"✓ Trackio/WandB initialized successfully!\")\n",
                "print(\"✓ Hugging Face transformers loaded!\")\n",
                "print(f\"Dashboard URL: {wandb.run.get_url()}\")\n",
                "\n",
                "# Load data\n",
                "train_df = pd.read_csv(r'c:/Users/meekg/Downloads/sep-25-dl-gen-ai-nppe-2/train.csv')\n",
                "test_df = pd.read_csv(r'c:/Users/meekg/Downloads/sep-25-dl-gen-ai-nppe-2/test.csv')\n",
                "\n",
                "# Log dataset info to WandB\n",
                "wandb.log({\n",
                "    \"train_size\": len(train_df),\n",
                "    \"test_size\": len(test_df)\n",
                "})\n",
                "\n",
                "# For c3 and c8\n",
                "train_c3 = train_df['sst3']\n",
                "train_c8 = train_df['sst8']\n",
                "train_seq = train_df['seq']\n",
                "test_seq = test_df['seq']\n",
                "\n",
                "# Encode labels for c3 and c8\n",
                "c3_labels = sorted(list(set(''.join(train_c3))))\n",
                "c8_labels = sorted(list(set(''.join(train_c8))))\n",
                "c3_encoder = LabelEncoder().fit(c3_labels)\n",
                "c8_encoder = LabelEncoder().fit(c8_labels)\n",
                "\n",
                "print(f\"C3 classes: {c3_labels} (count: {len(c3_labels)})\")\n",
                "print(f\"C8 classes: {c8_labels} (count: {len(c8_labels)})\")\n",
                "\n",
                "# Max sequence length for padding\n",
                "max_len = max(train_seq.apply(len).max(), test_seq.apply(len).max())\n",
                "print(f\"Max sequence length: {max_len}\")\n",
                "\n",
                "# Amino acid vocabulary\n",
                "vocab = sorted(list(set(''.join(train_seq) + ''.join(test_seq))))\n",
                "vocab_dict = {aa: i+1 for i, aa in enumerate(vocab)}  # 0 is padding\n",
                "vocab_size = len(vocab_dict) + 1\n",
                "\n",
                "wandb.config.update({\n",
                "    \"vocab_size\": vocab_size,\n",
                "    \"max_len\": max_len,\n",
                "    \"c3_classes\": len(c3_labels),\n",
                "    \"c8_classes\": len(c8_labels)\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ed3b036",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions for encoding sequences and labels\n",
                "def encode_sequence(seq, vocab_dict, max_len):\n",
                "    arr = np.zeros(max_len, dtype=int)\n",
                "    for i, aa in enumerate(seq[:max_len]):\n",
                "        arr[i] = vocab_dict.get(aa, 0)\n",
                "    return arr\n",
                "\n",
                "def encode_labels(labels, encoder, max_len):\n",
                "    arr = np.full(max_len, -1, dtype=int)\n",
                "    for i, l in enumerate(labels[:max_len]):\n",
                "        arr[i] = encoder.transform([l])[0]\n",
                "    return arr\n",
                "\n",
                "# Custom Dataset\n",
                "class ProteinDataset(Dataset):\n",
                "    def __init__(self, seqs, labels, vocab_dict, encoder, max_len):\n",
                "        self.seqs = seqs\n",
                "        self.labels = labels\n",
                "        self.vocab_dict = vocab_dict\n",
                "        self.encoder = encoder\n",
                "        self.max_len = max_len\n",
                "    def __len__(self):\n",
                "        return len(self.seqs)\n",
                "    def __getitem__(self, idx):\n",
                "        seq = encode_sequence(self.seqs.iloc[idx], self.vocab_dict, self.max_len)\n",
                "        if self.labels is not None:\n",
                "            label = encode_labels(self.labels.iloc[idx], self.encoder, self.max_len)\n",
                "            return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
                "        else:\n",
                "            return torch.tensor(seq, dtype=torch.long)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2adb5701",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enhanced BiLSTM Model with Dropout and LayerNorm\n",
                "def make_bilstm_model(vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.3):\n",
                "    class BiLSTM(nn.Module):\n",
                "        def __init__(self):\n",
                "            super().__init__()\n",
                "            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "            self.dropout1 = nn.Dropout(dropout)\n",
                "            # Stack 2 LSTM layers for better performance\n",
                "            self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, \n",
                "                              batch_first=True, bidirectional=True, dropout=dropout)\n",
                "            self.dropout2 = nn.Dropout(dropout)\n",
                "            self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
                "            self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
                "        def forward(self, x):\n",
                "            x = self.embedding(x)\n",
                "            x = self.dropout1(x)\n",
                "            out, _ = self.lstm(x)\n",
                "            out = self.dropout2(out)\n",
                "            out = self.layer_norm(out)\n",
                "            out = self.fc(out)\n",
                "            return out\n",
                "    return BiLSTM()\n",
                "\n",
                "# Training and evaluation helpers with WandB logging\n",
                "def train_epoch(model, loader, optimizer, criterion, device, epoch, model_name):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    batch_count = 0\n",
                "    for x, y in loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        out = model(x)\n",
                "        out = out.permute(0, 2, 1)  # (batch, classes, seq)\n",
                "        loss = criterion(out, y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "        batch_count += 1\n",
                "        \n",
                "        # Log batch loss to WandB every 50 batches\n",
                "        if batch_count % 50 == 0:\n",
                "            wandb.log({f\"{model_name}_batch_loss\": loss.item()})\n",
                "    \n",
                "    avg_loss = total_loss / len(loader)\n",
                "    # Log epoch metrics to WandB\n",
                "    wandb.log({\n",
                "        f\"{model_name}_epoch\": epoch,\n",
                "        f\"{model_name}_epoch_loss\": avg_loss\n",
                "    })\n",
                "    return avg_loss\n",
                "\n",
                "def predict(model, loader, device):\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    with torch.no_grad():\n",
                "        for x in loader:\n",
                "            x = x.to(device)\n",
                "            out = model(x)\n",
                "            pred = out.argmax(-1).cpu().numpy()\n",
                "            preds.append(pred)\n",
                "    return np.concatenate(preds, axis=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed70db15",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare datasets and dataloaders for c3 and c8\n",
                "BATCH_SIZE = wandb.config.batch_size\n",
                "EMBED_DIM = wandb.config.embed_dim\n",
                "HIDDEN_DIM = wandb.config.hidden_dim\n",
                "EPOCHS = wandb.config.epochs\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "wandb.config.update({\"device\": str(DEVICE)})\n",
                "\n",
                "# For c3\n",
                "train_c3_dataset = ProteinDataset(train_seq, train_c3, vocab_dict, c3_encoder, max_len)\n",
                "train_c3_loader = DataLoader(train_c3_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_c3_dataset = ProteinDataset(test_seq, None, vocab_dict, c3_encoder, max_len)\n",
                "test_c3_loader = DataLoader(test_c3_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "# For c8\n",
                "train_c8_dataset = ProteinDataset(train_seq, train_c8, vocab_dict, c8_encoder, max_len)\n",
                "train_c8_loader = DataLoader(train_c8_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_c8_dataset = ProteinDataset(test_seq, None, vocab_dict, c8_encoder, max_len)\n",
                "test_c8_loader = DataLoader(test_c8_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(\"✓ Dataloaders created successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bfdeb10",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and predict for c3\n",
                "print(\"=\" * 50)\n",
                "print(\"Training C3 Model\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "model_c3 = make_bilstm_model(vocab_size, EMBED_DIM, HIDDEN_DIM, len(c3_labels)).to(DEVICE)\n",
                "optimizer_c3 = torch.optim.Adam(model_c3.parameters(), lr=wandb.config.learning_rate)\n",
                "criterion_c3 = nn.CrossEntropyLoss(ignore_index=-1)\n",
                "\n",
                "# Watch model with WandB\n",
                "wandb.watch(model_c3, criterion_c3, log=\"all\", log_freq=100)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    loss = train_epoch(model_c3, train_c3_loader, optimizer_c3, criterion_c3, DEVICE, epoch+1, \"c3\")\n",
                "    print(f'Epoch {epoch+1}/{EPOCHS} c3 loss: {loss:.4f}')\n",
                "\n",
                "print(\"\\n✓ C3 training complete! Making predictions...\")\n",
                "c3_preds = predict(model_c3, test_c3_loader, DEVICE)\n",
                "c3_preds_str = [''.join(c3_encoder.inverse_transform(row[:len(seq)])) for row, seq in zip(c3_preds, test_seq)]\n",
                "print(\"✓ C3 predictions generated!\")\n",
                "\n",
                "# Save C3 model\n",
                "torch.save(model_c3.state_dict(), 'model_c3.pth')\n",
                "wandb.save('model_c3.pth')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8_training",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train and predict for c8\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"Training C8 Model\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "model_c8 = make_bilstm_model(vocab_size, EMBED_DIM, HIDDEN_DIM, len(c8_labels)).to(DEVICE)\n",
                "optimizer_c8 = torch.optim.Adam(model_c8.parameters(), lr=wandb.config.learning_rate)\n",
                "criterion_c8 = nn.CrossEntropyLoss(ignore_index=-1)\n",
                "\n",
                "# Watch model with WandB\n",
                "wandb.watch(model_c8, criterion_c8, log=\"all\", log_freq=100)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    loss = train_epoch(model_c8, train_c8_loader, optimizer_c8, criterion_c8, DEVICE, epoch+1, \"c8\")\n",
                "    print(f'Epoch {epoch+1}/{EPOCHS} c8 loss: {loss:.4f}')\n",
                "\n",
                "print(\"\\n✓ C8 training complete! Making predictions...\")\n",
                "c8_preds = predict(model_c8, test_c8_loader, DEVICE)\n",
                "c8_preds_str = [''.join(c8_encoder.inverse_transform(row[:len(seq)])) for row, seq in zip(c8_preds, test_seq)]\n",
                "print(\"✓ C8 predictions generated!\")\n",
                "\n",
                "# Save C8 model\n",
                "torch.save(model_c8.state_dict(), 'model_c8.pth')\n",
                "wandb.save('model_c8.pth')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3a283d17",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate submission file\n",
                "submission = pd.DataFrame({\n",
                "    'id': test_df['id'],\n",
                "    'sst3': c3_preds_str,\n",
                "    'sst8': c8_preds_str\n",
                "})\n",
                "\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "print('\\n' + '=' * 50)\n",
                "print('✓ submission.csv generated successfully!')\n",
                "print('=' * 50)\n",
                "print(f'Submission shape: {submission.shape}')\n",
                "print('\\nFirst few rows:')\n",
                "print(submission.head())\n",
                "\n",
                "# Log submission to WandB\n",
                "wandb.save('submission.csv')\n",
                "submission_artifact = wandb.Artifact('submission', type='dataset')\n",
                "submission_artifact.add_file('submission.csv')\n",
                "wandb.log_artifact(submission_artifact)\n",
                "\n",
                "print(f\"\\n✓ Submission logged to WandB dashboard: {wandb.run.get_url()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "finish",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Finish WandB run\n",
                "wandb.finish()\n",
                "print(\"\\n✓ WandB tracking complete! Check your dashboard for metrics and visualizations.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}