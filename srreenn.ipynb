{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:25.403474Z",
     "iopub.status.busy": "2025-12-13T17:48:25.403255Z",
     "iopub.status.idle": "2025-12-13T17:48:26.839163Z",
     "shell.execute_reply": "2025-12-13T17:48:26.838416Z",
     "shell.execute_reply.started": "2025-12-13T17:48:25.403451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bilstm-q8/pytorch/default/1/bilstm_q8_best.pt\n",
      "/kaggle/input/bilstm-q3/pytorch/default/1/bilstm_q3_best.pt\n",
      "/kaggle/input/sep-25-dl-gen-ai-nppe-2/sample_submission.csv\n",
      "/kaggle/input/sep-25-dl-gen-ai-nppe-2/train.csv\n",
      "/kaggle/input/sep-25-dl-gen-ai-nppe-2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:26.840905Z",
     "iopub.status.busy": "2025-12-13T17:48:26.840291Z",
     "iopub.status.idle": "2025-12-13T17:48:50.054332Z",
     "shell.execute_reply": "2025-12-13T17:48:50.053571Z",
     "shell.execute_reply.started": "2025-12-13T17:48:26.840876Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trackio -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:50.055488Z",
     "iopub.status.busy": "2025-12-13T17:48:50.055256Z",
     "iopub.status.idle": "2025-12-13T17:48:56.170242Z",
     "shell.execute_reply": "2025-12-13T17:48:56.169700Z",
     "shell.execute_reply.started": "2025-12-13T17:48:50.055465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:56.171290Z",
     "iopub.status.busy": "2025-12-13T17:48:56.170985Z",
     "iopub.status.idle": "2025-12-13T17:48:56.253372Z",
     "shell.execute_reply": "2025-12-13T17:48:56.252676Z",
     "shell.execute_reply.started": "2025-12-13T17:48:56.171271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Huggingface Access Adding token to environment variable\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "import os\n",
    "os.environ['HF_TOKEN']  = user_secrets.get_secret(\"dra_hf_access_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:56.255505Z",
     "iopub.status.busy": "2025-12-13T17:48:56.255247Z",
     "iopub.status.idle": "2025-12-13T17:48:56.913472Z",
     "shell.execute_reply": "2025-12-13T17:48:56.912889Z",
     "shell.execute_reply.started": "2025-12-13T17:48:56.255489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/sep-25-dl-gen-ai-nppe-2/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/sep-25-dl-gen-ai-nppe-2/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:56.914290Z",
     "iopub.status.busy": "2025-12-13T17:48:56.914116Z",
     "iopub.status.idle": "2025-12-13T17:48:56.920010Z",
     "shell.execute_reply": "2025-12-13T17:48:56.919212Z",
     "shell.execute_reply.started": "2025-12-13T17:48:56.914275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'seq', 'sst8', 'sst3'], dtype='object')\n",
      "Index(['id', 'seq'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:56.921035Z",
     "iopub.status.busy": "2025-12-13T17:48:56.920785Z",
     "iopub.status.idle": "2025-12-13T17:48:56.995403Z",
     "shell.execute_reply": "2025-12-13T17:48:56.994880Z",
     "shell.execute_reply.started": "2025-12-13T17:48:56.921011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>sst8</th>\n",
       "      <th>sst3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GVGLEGGVQLSPARTRGPEFAAPEQAG</td>\n",
       "      <td>CCCCCCCCSCCCCCCGGGCCCCCCCCC</td>\n",
       "      <td>CCCCCCCCCCCCCCCHHHCCCCCCCCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NHGKVKIEHTKWNVEYKVTYNRNVFANHIRSGELASNGYHTTRRTA...</td>\n",
       "      <td>CEEEEEECCTTTEEEEEEEEEEEEEEEEEEEEECSCCSSSCCCCEE...</td>\n",
       "      <td>CEEEEEECCCCCEEEEEEEEEEEEEEEEEEEEECCCCCCCCCCCEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>EMRKMLADWKGLSKSDGMLSSEGRTKALWLGEANFSYVPKLDPRAS...</td>\n",
       "      <td>CTHHHHHHHHHSGGGCCCCCCCCCCCCEEECSSCEEEEEEETTCGG...</td>\n",
       "      <td>CCHHHHHHHHHCHHHCCCCCCCCCCCCEEECCCCEEEEEEECCCHH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>QDNKNGWQIRSDDVWGPDTKDSIQTVEGTRDNVVVYKGPSGYVTAP...</td>\n",
       "      <td>CCCCCCEECTTCCBCTTCTTCCEEEEBCTTTEEEEEETTEEEEEEE...</td>\n",
       "      <td>CCCCCCEECCCCCECCCCCCCCEEEEECCCCEEEEEECCEEEEEEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>VPNSRDGGGGNHWNVEFGQLIALIGAAICGVIGGALGGFTAAGSCG...</td>\n",
       "      <td>CEEEEEECCCHHHHHHTHHHHHHHHHHHHHHHHSCCTTCCCCSSCC...</td>\n",
       "      <td>CEEEEEECCCHHHHHHCHHHHHHHHHHHHHHHHCCCCCCCCCCCCC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                seq  \\\n",
       "0   0                        GVGLEGGVQLSPARTRGPEFAAPEQAG   \n",
       "1   1  NHGKVKIEHTKWNVEYKVTYNRNVFANHIRSGELASNGYHTTRRTA...   \n",
       "2   2  EMRKMLADWKGLSKSDGMLSSEGRTKALWLGEANFSYVPKLDPRAS...   \n",
       "3   3  QDNKNGWQIRSDDVWGPDTKDSIQTVEGTRDNVVVYKGPSGYVTAP...   \n",
       "4   4  VPNSRDGGGGNHWNVEFGQLIALIGAAICGVIGGALGGFTAAGSCG...   \n",
       "\n",
       "                                                sst8  \\\n",
       "0                        CCCCCCCCSCCCCCCGGGCCCCCCCCC   \n",
       "1  CEEEEEECCTTTEEEEEEEEEEEEEEEEEEEEECSCCSSSCCCCEE...   \n",
       "2  CTHHHHHHHHHSGGGCCCCCCCCCCCCEEECSSCEEEEEEETTCGG...   \n",
       "3  CCCCCCEECTTCCBCTTCTTCCEEEEBCTTTEEEEEETTEEEEEEE...   \n",
       "4  CEEEEEECCCHHHHHHTHHHHHHHHHHHHHHHHSCCTTCCCCSSCC...   \n",
       "\n",
       "                                                sst3  \n",
       "0                        CCCCCCCCCCCCCCCHHHCCCCCCCCC  \n",
       "1  CEEEEEECCCCCEEEEEEEEEEEEEEEEEEEEECCCCCCCCCCCEE...  \n",
       "2  CCHHHHHHHHHCHHHCCCCCCCCCCCCEEECCCCEEEEEEECCCHH...  \n",
       "3  CCCCCCEECCCCCECCCCCCCCEEEEECCCCEEEEEECCEEEEEEE...  \n",
       "4  CEEEEEECCCHHHHHHCHHHHHHHHHHHHHHHHCCCCCCCCCCCCC...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:56.996255Z",
     "iopub.status.busy": "2025-12-13T17:48:56.996046Z",
     "iopub.status.idle": "2025-12-13T17:48:57.003161Z",
     "shell.execute_reply": "2025-12-13T17:48:57.002554Z",
     "shell.execute_reply.started": "2025-12-13T17:48:56.996230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FFKGSYQKVSNQLLYQANQIQDQTGTITIIRDESGELPEDIKISAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ATTKKNSPFPKVEEAYVSGDANITLFIKRGAHIAQNISSPYVGLDK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VRALMLELRSGVREALDALGGVWEITKYLFMVDVPNLESELAFLQR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HTWGEAAQEDFTRDIREFRRRISERAAAHPLIYLRNALIADATLRA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>RFLTVVLDAGLGVKEYHERCMAIYDHVDFHGISNGAEDLWILPGQV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                seq\n",
       "0   0  FFKGSYQKVSNQLLYQANQIQDQTGTITIIRDESGELPEDIKISAG...\n",
       "1   1  ATTKKNSPFPKVEEAYVSGDANITLFIKRGAHIAQNISSPYVGLDK...\n",
       "2   2  VRALMLELRSGVREALDALGGVWEITKYLFMVDVPNLESELAFLQR...\n",
       "3   3  HTWGEAAQEDFTRDIREFRRRISERAAAHPLIYLRNALIADATLRA...\n",
       "4   4  RFLTVVLDAGLGVKEYHERCMAIYDHVDFHGISNGAEDLWILPGQV..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:57.004195Z",
     "iopub.status.busy": "2025-12-13T17:48:57.003954Z",
     "iopub.status.idle": "2025-12-13T17:48:57.284450Z",
     "shell.execute_reply": "2025-12-13T17:48:57.283642Z",
     "shell.execute_reply.started": "2025-12-13T17:48:57.004180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Length consistency check\n",
    "bad_rows = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    if not (len(row['seq']) == len(row['sst8']) == len(row['sst3'])):\n",
    "        bad_rows.append(i)\n",
    "\n",
    "print(\"Bad rows:\", len(bad_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:57.285548Z",
     "iopub.status.busy": "2025-12-13T17:48:57.285271Z",
     "iopub.status.idle": "2025-12-13T17:48:57.288952Z",
     "shell.execute_reply": "2025-12-13T17:48:57.288165Z",
     "shell.execute_reply.started": "2025-12-13T17:48:57.285533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Amino acid vocabulary\n",
    "# AMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "# aa2idx['<UNK>'] = len(aa2idx)  # new index for unknown\n",
    "# aa2idx = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}  # 0 reserved for padding\n",
    "# idx2aa = {i: aa for aa, i in aa2idx.items()}\n",
    "\n",
    "\n",
    "# VOCAB_SIZE = len(aa2idx) + 1  # +1 for padding\n",
    "# print(\"Vocab size:\", VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# # Q8 label vocabulary\n",
    "# Q8_LABELS = sorted(list(set(\"\".join(train_df[\"sst8\"]))))\n",
    "\n",
    "# q8_2idx = {l: i for i, l in enumerate(Q8_LABELS)}\n",
    "# idx2q8 = {i: l for l, i in q8_2idx.items()}\n",
    "\n",
    "# print(\"Q8 labels:\", Q8_LABELS)\n",
    "\n",
    "\n",
    "# # Q3 label vocabulary\n",
    "# Q3_LABELS = sorted(list(set(\"\".join(train_df[\"sst3\"]))))\n",
    "\n",
    "# q3_2idx = {l: i for i, l in enumerate(Q3_LABELS)}\n",
    "# idx2q3 = {i: l for l, i in q3_2idx.items()}\n",
    "\n",
    "# print(\"Q3 labels:\", Q3_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:57.289978Z",
     "iopub.status.busy": "2025-12-13T17:48:57.289633Z",
     "iopub.status.idle": "2025-12-13T17:48:57.303858Z",
     "shell.execute_reply": "2025-12-13T17:48:57.303177Z",
     "shell.execute_reply.started": "2025-12-13T17:48:57.289953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df, task=\"q8\"):\n",
    "        self.seqs = df[\"seq\"].values\n",
    "        \n",
    "        if task == \"q8\":\n",
    "            self.labels = df[\"sst8\"].values\n",
    "            self.label2idx = q8_2idx\n",
    "        else:\n",
    "            self.labels = df[\"sst3\"].values\n",
    "            self.label2idx = q3_2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        lab = self.labels[idx]\n",
    "\n",
    "        seq_encoded = torch.tensor(\n",
    "            [aa2idx.get(a, aa2idx['<UNK>']) for a in seq],  # fix KeyError\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        lab_encoded = torch.tensor(\n",
    "            [self.label2idx[c] for c in lab],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        return seq_encoded, lab_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:48:57.304809Z",
     "iopub.status.busy": "2025-12-13T17:48:57.304581Z",
     "iopub.status.idle": "2025-12-13T17:49:04.372771Z",
     "shell.execute_reply": "2025-12-13T17:49:04.372100Z",
     "shell.execute_reply.started": "2025-12-13T17:48:57.304794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TrackIO available\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TrackIO import\n",
    "try:\n",
    "    import trackio\n",
    "    TRACKIO_AVAILABLE = True\n",
    "    print(\"✓ TrackIO available\")\n",
    "except ImportError:\n",
    "    TRACKIO_AVAILABLE = False\n",
    "    print(\"⚠️ TrackIO not available - install with: pip install trackio\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.373759Z",
     "iopub.status.busy": "2025-12-13T17:49:04.373516Z",
     "iopub.status.idle": "2025-12-13T17:49:04.415984Z",
     "shell.execute_reply": "2025-12-13T17:49:04.415375Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.373734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train samples: 6535 | Val samples: 727\n",
      "AA vocab size: 22\n",
      "Q8 classes: 8 -> ['B', 'C', 'E', 'G', 'H', 'I', 'S', 'T']\n",
      "Q3 classes: 3 -> ['C', 'E', 'H']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1️⃣ DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Split train/validation\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42, shuffle=True)\n",
    "print(f\"\\nTrain samples: {len(train_data)} | Val samples: {len(val_data)}\")\n",
    "\n",
    "# Build vocabularies\n",
    "aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "aa2idx = {aa: i+1 for i, aa in enumerate(aa_list)}\n",
    "aa2idx['<PAD>'] = 0\n",
    "aa2idx['<UNK>'] = 21\n",
    "\n",
    "Q8_LABELS = sorted(list(set(\"\".join(train_df[\"sst8\"]))))\n",
    "q8_2idx = {l: i for i, l in enumerate(Q8_LABELS)}\n",
    "idx2q8 = {i: l for l, i in q8_2idx.items()}\n",
    "\n",
    "Q3_LABELS = sorted(list(set(\"\".join(train_df[\"sst3\"]))))\n",
    "q3_2idx = {l: i for i, l in enumerate(Q3_LABELS)}\n",
    "idx2q3 = {i: l for l, i in q3_2idx.items()}\n",
    "\n",
    "print(f\"AA vocab size: 22\")\n",
    "print(f\"Q8 classes: {len(q8_2idx)} -> {Q8_LABELS}\")\n",
    "print(f\"Q3 classes: {len(q3_2idx)} -> {Q3_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.418526Z",
     "iopub.status.busy": "2025-12-13T17:49:04.418335Z",
     "iopub.status.idle": "2025-12-13T17:49:04.425195Z",
     "shell.execute_reply": "2025-12-13T17:49:04.424589Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.418512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2️⃣ DATASET & DATALOADER\n",
    "# =============================================================================\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df, task=\"q8\"):\n",
    "        self.seqs = df[\"seq\"].values\n",
    "        self.task = task\n",
    "        if task == \"q8\":\n",
    "            self.labels = df[\"sst8\"].values\n",
    "            self.label2idx = q8_2idx\n",
    "        else:\n",
    "            self.labels = df[\"sst3\"].values\n",
    "            self.label2idx = q3_2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        lab = self.labels[idx]\n",
    "        \n",
    "        seq_encoded = torch.tensor([aa2idx.get(a, 21) for a in seq], dtype=torch.long)\n",
    "        lab_encoded = torch.tensor([self.label2idx.get(c, 0) for c in lab], dtype=torch.long)\n",
    "        \n",
    "        return seq_encoded, lab_encoded, len(seq_encoded)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return seqs_padded, labels_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.426209Z",
     "iopub.status.busy": "2025-12-13T17:49:04.425930Z",
     "iopub.status.idle": "2025-12-13T17:49:04.445081Z",
     "shell.execute_reply": "2025-12-13T17:49:04.444523Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.426173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3️⃣ MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,\n",
    "                 num_layers=2, cnn_filters=[64, 128], kernel_sizes=[3,5], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer for amino acids\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.dropout_embed = nn.Dropout(dropout)\n",
    "        \n",
    "        # -------------------\n",
    "        # CNN for local patterns\n",
    "        # -------------------\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=f, kernel_size=k, padding=k//2)\n",
    "            for f, k in zip(cnn_filters, kernel_sizes)\n",
    "        ])\n",
    "        \n",
    "        # -------------------\n",
    "        # BiLSTM for long-range dependencies\n",
    "        # -------------------\n",
    "        lstm_input_dim = sum(cnn_filters)  # CNN output channels concatenated\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim*2)\n",
    "        self.dropout_out = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # -------------------\n",
    "        # Embedding\n",
    "        # x: [B, L]\n",
    "        # -------------------\n",
    "        embed = self.embedding(x)       # [B, L, embed_dim]\n",
    "        embed = self.dropout_embed(embed)\n",
    "        \n",
    "        # -------------------\n",
    "        # CNN expects [B, embed_dim, L]\n",
    "        # -------------------\n",
    "        cnn_input = embed.transpose(1,2)\n",
    "        cnn_outs = [F.relu(conv(cnn_input)) for conv in self.convs]  # list of [B, out_channels, L]\n",
    "        cnn_out = torch.cat(cnn_outs, dim=1).transpose(1,2)          # [B, L, sum(out_channels)]\n",
    "        \n",
    "        # -------------------\n",
    "        # Pack for LSTM\n",
    "        # -------------------\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(cnn_out, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # [B, L, hidden*2]\n",
    "        \n",
    "        # -------------------\n",
    "        # Layer norm + dropout + classifier\n",
    "        # -------------------\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.dropout_out(lstm_out)\n",
    "        logits = self.fc(lstm_out)  # [B, L, output_dim]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.446065Z",
     "iopub.status.busy": "2025-12-13T17:49:04.445848Z",
     "iopub.status.idle": "2025-12-13T17:49:04.464534Z",
     "shell.execute_reply": "2025-12-13T17:49:04.463938Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.446052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4️⃣ TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(task=\"q8\", num_epochs=20, batch_size=64, lr=1e-3, use_trackio=True):\n",
    "    \"\"\"Train model for either Q8 or Q3 task using BiLSTM+CNN\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training BiLSTM+CNN for {task.upper()} task\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize TrackIO\n",
    "    tracker = None\n",
    "    if use_trackio and TRACKIO_AVAILABLE:\n",
    "        try:\n",
    "            trackio.init(\n",
    "                project=\"25-t3-nppe2\",\n",
    "                group=f\"bilstm_cnn_{task}\",\n",
    "                name=f\"bilstm_cnn_{task}_run\"\n",
    "            )\n",
    "            trackio.config({\n",
    "                \"model\": \"BiLSTM_CNN\",\n",
    "                \"task\": task,\n",
    "                \"vocab_size\": 22,\n",
    "                \"embed_dim\": 128,\n",
    "                \"hidden_dim\": 256,\n",
    "                \"num_layers\": 2,\n",
    "                \"cnn_filters\": [64,128],\n",
    "                \"kernel_sizes\": [3,5],\n",
    "                \"dropout\": 0.3,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"output_classes\": len(q8_2idx) if task == \"q8\" else len(q3_2idx)\n",
    "            })\n",
    "            tracker = True\n",
    "            print(\"✓ TrackIO initialized\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ TrackIO initialization failed: {e}\")\n",
    "            tracker = None\n",
    "    \n",
    "    # Prepare data\n",
    "    train_dataset = ProteinDataset(train_data, task=task)\n",
    "    val_dataset = ProteinDataset(val_data, task=task)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output_dim = len(q8_2idx) if task == \"q8\" else len(q3_2idx)\n",
    "\n",
    "    model = BiLSTM_CNN(\n",
    "        vocab_size=22,\n",
    "        embed_dim=128,\n",
    "        hidden_dim=256,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=2,\n",
    "        cnn_filters=[64,128],\n",
    "        kernel_sizes=[3,7,11],\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Resume from checkpoint if exists\n",
    "    checkpoint_path = f\"bilstm_cnn_{task}_checkpoint.pt\"\n",
    "    start_epoch = 0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_f1 = checkpoint['best_f1']\n",
    "        print(f\"✓ Resuming from epoch {start_epoch}, best F1: {best_f1:.4f}\")\n",
    "     \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for seqs, labels, lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            seqs, labels, lengths = seqs.to(device), labels.to(device), lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(seqs, lengths)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for seqs, labels, lengths in val_loader:\n",
    "                seqs, labels, lengths = seqs.to(device), labels.to(device), lengths.to(device)\n",
    "                \n",
    "                logits = model(seqs, lengths)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                mask = labels.view(-1) != -100\n",
    "                all_preds.extend(preds.view(-1)[mask].cpu().numpy())\n",
    "                all_labels.extend(labels.view(-1)[mask].cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val F1: {val_f1:.4f} | LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Log to TrackIO\n",
    "        if tracker:\n",
    "            trackio.log({\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"val_f1\": val_f1,\n",
    "                \"learning_rate\": current_lr,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_f1': best_f1,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            torch.save(model.state_dict(), f\"bilstm_cnn_{task}_best.pt\")\n",
    "            print(f\"  → Best model saved (F1: {best_f1:.4f})\")\n",
    "            \n",
    "            # Log best metric to TrackIO\n",
    "            if tracker:\n",
    "                trackio.log({\n",
    "                    \"best_val_f1\": best_f1\n",
    "                })\n",
    "    \n",
    "    # Final summary to TrackIO\n",
    "    if tracker:\n",
    "        trackio.log({\n",
    "            \"final_best_f1\": best_f1,\n",
    "            \"total_epochs\": num_epochs,\n",
    "            \"task\": task\n",
    "        })\n",
    "        trackio.finish()\n",
    "        print(\"✓ TrackIO logging complete\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best F1: {best_f1:.4f}\")\n",
    "    return model, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.465509Z",
     "iopub.status.busy": "2025-12-13T17:49:04.465289Z",
     "iopub.status.idle": "2025-12-13T17:49:04.478517Z",
     "shell.execute_reply": "2025-12-13T17:49:04.477965Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.465485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5️⃣ INFERENCE FUNCTION\n",
    "# =============================================================================\n",
    "import os\n",
    "def predict(model, test_df, task=\"q8\", device=\"cuda\"):\n",
    "    \"\"\"Generate predictions for test set\"\"\"\n",
    "    \n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.seqs = df[\"seq\"].values\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.seqs)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            seq = self.seqs[idx]\n",
    "            seq_encoded = torch.tensor([aa2idx.get(a, 21) for a in seq], dtype=torch.long)\n",
    "            return seq_encoded, len(seq_encoded)\n",
    "    \n",
    "    def test_collate_fn(batch):\n",
    "        seqs, lengths = zip(*batch)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        return seqs_padded, lengths\n",
    "    \n",
    "    test_dataset = TestDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=test_collate_fn)\n",
    "    \n",
    "    idx2label = idx2q8 if task == \"q8\" else idx2q3\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths in tqdm(test_loader, desc=f\"Predicting {task.upper()}\"):\n",
    "            seqs, lengths = seqs.to(device), lengths.to(device)\n",
    "            logits = model(seqs, lengths)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Convert to strings\n",
    "            for i, length in enumerate(lengths):\n",
    "                pred_seq = \"\".join([idx2label[idx.item()] for idx in preds[i][:length]])\n",
    "                all_predictions.append(pred_seq)\n",
    "    \n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:49:04.479595Z",
     "iopub.status.busy": "2025-12-13T17:49:04.479298Z",
     "iopub.status.idle": "2025-12-13T18:29:29.568947Z",
     "shell.execute_reply": "2025-12-13T18:29:29.568177Z",
     "shell.execute_reply.started": "2025-12-13T17:49:04.479579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HF token set successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Q8 Epoch 1: 100%|██████████| 103/103 [00:58<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.4473 | Val F1: 0.2742 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.2742)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 2: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 1.2281 | Val F1: 0.2871 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.2871)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 3: 100%|██████████| 103/103 [00:58<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 1.1816 | Val F1: 0.3055 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3055)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 4: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 1.1545 | Val F1: 0.3031 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 5: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 1.1293 | Val F1: 0.3148 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 6: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 1.1021 | Val F1: 0.3184 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3184)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 7: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 1.0851 | Val F1: 0.3262 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3262)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 8: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 1.0664 | Val F1: 0.3282 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3282)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 9: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 1.0474 | Val F1: 0.3276 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 10: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 1.0253 | Val F1: 0.3303 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3303)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 11: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 1.0046 | Val F1: 0.3338 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 12: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 0.9873 | Val F1: 0.3338 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 13: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 0.9626 | Val F1: 0.3346 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 14: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 0.9425 | Val F1: 0.3300 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 15: 100%|██████████| 103/103 [00:58<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss: 0.9218 | Val F1: 0.3408 | LR: 0.001000\n",
      "  → Best model saved for q8 (F1: 0.3408)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 16: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Loss: 0.8989 | Val F1: 0.3339 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 17: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Loss: 0.8831 | Val F1: 0.3365 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 18: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Loss: 0.8565 | Val F1: 0.3372 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 19: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Loss: 0.8156 | Val F1: 0.3349 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q8 Epoch 20: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Loss: 0.7922 | Val F1: 0.3403 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 1: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.9425 | Val F1: 0.6445 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.6445)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 2: 100%|██████████| 103/103 [00:58<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.7530 | Val F1: 0.6717 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.6717)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 3: 100%|██████████| 103/103 [00:58<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.7142 | Val F1: 0.6872 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.6872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 4: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.6988 | Val F1: 0.6904 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.6904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 5: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.6745 | Val F1: 0.6879 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 6: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.6619 | Val F1: 0.7053 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.7053)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 7: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.6457 | Val F1: 0.7074 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.7074)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 8: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.6367 | Val F1: 0.7104 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.7104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 9: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.6203 | Val F1: 0.7103 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 10: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.6080 | Val F1: 0.7086 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 11: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss: 0.5954 | Val F1: 0.7128 | LR: 0.001000\n",
      "  → Best model saved for q3 (F1: 0.7128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 12: 100%|██████████| 103/103 [00:57<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss: 0.5829 | Val F1: 0.6999 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 13: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss: 0.5684 | Val F1: 0.7027 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 14: 100%|██████████| 103/103 [00:57<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss: 0.5540 | Val F1: 0.7046 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 15: 100%|██████████| 103/103 [00:57<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Loss: 0.5246 | Val F1: 0.7082 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 16: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Loss: 0.5098 | Val F1: 0.7066 | LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 17: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Loss: 0.4965 | Val F1: 0.6993 | LR: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 18: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Loss: 0.4776 | Val F1: 0.7023 | LR: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 19: 100%|██████████| 103/103 [00:57<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Loss: 0.4662 | Val F1: 0.7018 | LR: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q3 Epoch 20: 100%|██████████| 103/103 [00:58<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Loss: 0.4574 | Val F1: 0.7020 | LR: 0.000125\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Q8 F1: 0.3408\n",
      "Q3 F1: 0.7128\n",
      "Harmonic Mean: 0.4611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Q8: 100%|██████████| 29/29 [00:09<00:00,  3.00it/s]\n",
      "Predicting Q3: 100%|██████████| 29/29 [00:09<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Submission file created: submission.csv\n",
      "   ID                                               sst8  \\\n",
      "0   0  CCCTTHHHHHHHHHHHHHHHHHHCSEEEEEECCCSTCCCEEEEECC...   \n",
      "1   1  CCCCCCCCCCEEEEEEECSTTCEEEEEECTTHHHHHHHHCCCTHHH...   \n",
      "2   2  CTHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCCCSCTHHHHH...   \n",
      "3   3  CCCTTCHHHCHHHHHHHHHHHHHHHHHHHCCEEHHTHHHHHHHHHH...   \n",
      "4   4  CEEEEEESCCCTHHHHHHHHHHHHHHHCEEEEECSTTCEEEETSEE...   \n",
      "\n",
      "                                                sst3  \n",
      "0  CCCCCCHHHHHHHHHHHHHHHHHCCEEEEEECCCCCCCCEEEEECC...  \n",
      "1  CCCCCCCCCEEEEEEEECCCCEEEEEEECCCHHHHHHHHCCCCHHH...  \n",
      "2  CCHHHHHHHHHHHHHHHHHCHHHHHHHHHHHHHHHCCCCHHHHHHH...  \n",
      "3  CCCCCHHHHHHHHHHHHHHHHHHHHHHHCCCEEECCHHHHHHHHHH...  \n",
      "4  CEEEEEECCCCCHHHHHHHHHHHHHHHCEEEEECCCCCEEEECCCE...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6️⃣ MAIN EXECUTION FOR KAGGLE - TRAIN AND SAVE MODELS\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: HF token\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"dra_hf_access_token\")\n",
    "    print(\"✓ HF token set successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ HF token not set: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING FUNCTION (SAFE, NO CHECKPOINTS)\n",
    "wandb: (1) Create a W&B account\n",
    "wandb: (2) Use an existing W&B account\n",
    "wandb: (3) Don't visualize my results\n",
    "wandb: Enter your choice:\n",
    "✓ Closed any existing wandb run\n",
    "✓ No existing wandb session found\n",
    "\n",
    "============================================================\n",
    "Please login with YOUR wandb API key:\n",
    "============================================================\n",
    "wandb: You chose 'Use an existing W&B account'\n",
    "wandb: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
    "wandb: Find your API key here: https://wandb.ai/authorize\n",
    "wandb: Paste an API key from your profile and hit enter:wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\meekg\\_netrc\n",
    "wandb: Currently logged in as: 23f2002825 (23f2002825-iit-madras) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
    "Tracking run with wandb version 0.23.1\n",
    "Run data is saved locally in c:\\Users\\meekg\\Downloads\\sep-25-dl-gen-ai-nppe-2\\wandb\\run-20251216_004517-ai5ufau2\n",
    "Syncing run bilstm-sst-prediction to Weights & Biases (docs)\n",
    "View project at https://wandb.ai/23f2002825-iit-madras/protein-secondary-structure\n",
    "View run at https://wandb.ai/23f2002825-iit-madras/protein-secondary-structure/runs/ai5ufau2\n",
    "wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n",
    "\n",
    "✓ WandB initialized successfully with YOUR account!\n",
    "✓ Dashboard URL: https://wandb.ai/23f2002825-iit-madras/protein-secondary-structure/runs/ai5ufau2\n",
    "✓ Hugging Face transformers loaded!\n",
    "\n",
    "Loading datasets...\n",
    "✓ Train dataset: 7262 samples\n",
    "✓ Test dataset: 1816 samples\n",
    "\n",
    "C3 classes: ['C', 'E', 'H'] (count: 3)\n",
    "C8 classes: ['B', 'C', 'E', 'G', 'H', 'I', 'S', 'T'] (count: 8)\n",
    "Max sequence length: 1632\n",
    "\n",
    "============================================================\n",
    "✓ Cell 2 complete - Ready to proceed with training!\n",
    "============================================================\n",
    "Using device: cuda\n",
    "✓ Dataloaders created successfully!\n",
    "==================================================\n",
    "Training C3 Model\n",
    "==================================================\n",
    "Epoch 1/10 c3 loss: 0.8535\n",
    "Epoch 2/10 c3 loss: 0.7934\n",
    "Epoch 3/10 c3 loss: 0.7614\n",
    "Epoch 4/10 c3 loss: 0.7076\n",
    "Epoch 5/10 c3 loss: 0.6918\n",
    "Epoch 6/10 c3 loss: 0.6784\n",
    "Epoch 7/10 c3 loss: 0.6703\n",
    "Epoch 8/10 c3 loss: 0.6609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T18:29:29.570092Z",
     "iopub.status.busy": "2025-12-13T18:29:29.569799Z",
     "iopub.status.idle": "2025-12-13T18:29:29.573857Z",
     "shell.execute_reply": "2025-12-13T18:29:29.573071Z",
     "shell.execute_reply.started": "2025-12-13T18:29:29.570065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# If you want to upload just the model file\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Create temp directory for Q8\n",
    "# os.makedirs(\"q8_model\", exist_ok=True)\n",
    "# shutil.copy(\"bilstm_q8_best.pt\", \"q8_model/bilstm_q8_best.pt\")\n",
    "\n",
    "# kagglehub.model_upload(\n",
    "#     handle=\"sreebalajis/bilstm-q8/pytorch/default\",\n",
    "#     local_model_dir=\"q8_model\",\n",
    "#     version_notes=\"BiLSTM Q8 model\"\n",
    "# )\n",
    "\n",
    "# # Same for Q3\n",
    "# os.makedirs(\"q3_model\", exist_ok=True)\n",
    "# shutil.copy(\"bilstm_q3_best.pt\", \"q3_model/bilstm_q3_best.pt\")\n",
    "\n",
    "# kagglehub.model_upload(\n",
    "#     handle=\"sreebalajis/bilstm-q3/pytorch/default\",\n",
    "#     local_model_dir=\"q3_model\",\n",
    "#     version_notes=\"BiLSTM Q3 model\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T18:29:29.575233Z",
     "iopub.status.busy": "2025-12-13T18:29:29.575056Z",
     "iopub.status.idle": "2025-12-13T18:29:47.371928Z",
     "shell.execute_reply": "2025-12-13T18:29:47.371154Z",
     "shell.execute_reply.started": "2025-12-13T18:29:29.575220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from KaggleHub...\n",
      "✓ Q8 model loaded from: /kaggle/input/bilstm-q8/pytorch/default/1\n",
      "✓ Q3 model loaded from: /kaggle/input/bilstm-q3/pytorch/default/1\n",
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Q8: 100%|██████████| 29/29 [00:08<00:00,  3.41it/s]\n",
      "Predicting Q3: 100%|██████████| 29/29 [00:08<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Submission file created: submission.csv\n",
      "Total test samples: 1816\n",
      "\n",
      "First 5 predictions:\n",
      "   id                                               sst8  \\\n",
      "0   0  CCCCCHHHHHHHHHHHHHHHHHHCSEEEEEECCCTTCCCEEEEECT...   \n",
      "1   1  CCCCCCCCCCCEEEEEECSTTEEEEEEECTTCEEHHHCCCCEETCC...   \n",
      "2   2  CCHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHCSHHHHHHHHH...   \n",
      "3   3  CCCTHHHHHHHHHHHHHHHHHHHHHHHHCCEEEEHHHHHHHHHHHH...   \n",
      "4   4  CEEEEEEECCCTHHHHHHHHHHHHHHHCCEEEECSTTEEEEEESEE...   \n",
      "\n",
      "                                                sst3  \n",
      "0  CCCCCHHHHHHHHHHHHHHHHHHCCEEEEEECCCCCCCCCEEEECC...  \n",
      "1  CCCCCCCCCCCEEEEEECCCCEEEEEEECCCHHHHHHHHCCCCCHH...  \n",
      "2  CCHHHHHHHHHHHHHHHHHCCHHHHHHHHHHHCCCCCCHHHHHHHH...  \n",
      "3  CCCCCHHHHHHHHHHHHHHHHHHHHHHHHCCEEECCCHHHHHHHHH...  \n",
      "4  CEEEEEECCCCCHHHHHHHHHHHHHHHCCCCCCCCCCCEEEECCCE...  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 1️⃣ SETUP - Vocabularies and Model Architecture\n",
    "# =============================================================================\n",
    "\n",
    "# Vocabularies (same as training)\n",
    "aa_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "aa2idx = {aa: i+1 for i, aa in enumerate(aa_list)}\n",
    "aa2idx['<PAD>'] = 0\n",
    "aa2idx['<UNK>'] = 21\n",
    "\n",
    "Q8_LABELS = ['B', 'C', 'E', 'G', 'H', 'I', 'S', 'T']  # From your training\n",
    "q8_2idx = {l: i for i, l in enumerate(Q8_LABELS)}\n",
    "idx2q8 = {i: l for l, i in q8_2idx.items()}\n",
    "\n",
    "Q3_LABELS = ['C', 'E', 'H']  # Update if different\n",
    "q3_2idx = {l: i for i, l in enumerate(Q3_LABELS)}\n",
    "idx2q3 = {i: l for l, i in q3_2idx.items()}\n",
    "\n",
    "# Model Architecture (must match training!)\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# =============================================================================\n",
    "# 2️⃣ LOAD MODELS FROM KAGGLEHUB\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading models from KaggleHub...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Download Q8 model\n",
    "q8_path = kagglehub.model_download(\"sreebalajis/bilstm-q8/pyTorch/default\")\n",
    "model_q8 = BiLSTM(22, 128, 256, len(q8_2idx), num_layers=2, dropout=0.3).to(device)\n",
    "model_q8.load_state_dict(torch.load(f\"{q8_path}/bilstm_q8_best.pt\", map_location=device))\n",
    "model_q8.eval()\n",
    "print(f\"✓ Q8 model loaded from: {q8_path}\")\n",
    "\n",
    "# Download Q3 model\n",
    "q3_path = kagglehub.model_download(\"sreebalajis/bilstm-q3/pyTorch/default\")\n",
    "model_q3 = BiLSTM(22, 128, 256, len(q3_2idx), num_layers=2, dropout=0.3).to(device)\n",
    "model_q3.load_state_dict(torch.load(f\"{q3_path}/bilstm_q3_best.pt\", map_location=device))\n",
    "model_q3.eval()\n",
    "print(f\"✓ Q3 model loaded from: {q3_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3️⃣ PREDICTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def predict(model, test_df, task=\"q8\", device=\"cuda\"):\n",
    "    \"\"\"Generate predictions for test set\"\"\"\n",
    "    \n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, df):\n",
    "            self.seqs = df[\"seq\"].values\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.seqs)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            seq = self.seqs[idx]\n",
    "            seq_encoded = torch.tensor([aa2idx.get(a, 21) for a in seq], dtype=torch.long)\n",
    "            return seq_encoded, len(seq_encoded)\n",
    "    \n",
    "    def test_collate_fn(batch):\n",
    "        seqs, lengths = zip(*batch)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        return seqs_padded, lengths\n",
    "    \n",
    "    test_dataset = TestDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=test_collate_fn)\n",
    "    \n",
    "    idx2label = idx2q8 if task == \"q8\" else idx2q3\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seqs, lengths in tqdm(test_loader, desc=f\"Predicting {task.upper()}\"):\n",
    "            seqs, lengths = seqs.to(device), lengths.to(device)\n",
    "            logits = model(seqs, lengths)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            for i, length in enumerate(lengths):\n",
    "                pred_seq = \"\".join([idx2label[idx.item()] for idx in preds[i][:length]])\n",
    "                all_predictions.append(pred_seq)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# =============================================================================\n",
    "# 4️⃣ GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nGenerating predictions...\")\n",
    "q8_preds = predict(model_q8, test_df, task=\"q8\", device=device)\n",
    "q3_preds = predict(model_q3, test_df, task=\"q3\", device=device)\n",
    "\n",
    "# =============================================================================\n",
    "# 5️⃣ CREATE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'sst8': q8_preds,\n",
    "    'sst3': q3_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✓ Submission file created: submission.csv\")\n",
    "print(f\"Total test samples: {len(submission)}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14858151,
     "sourceId": 125543,
     "sourceType": "competition"
    },
    {
     "modelId": 532148,
     "modelInstanceId": 517498,
     "sourceId": 681943,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 532149,
     "modelInstanceId": 517499,
     "sourceId": 681944,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
